{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl_FIEj-auGq"
      },
      "source": [
        "## Training new Wake Words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q9wEuRdwY_E"
      },
      "source": [
        "# 1. Test Example Training Clip Generation\n",
        "Since openWakeWord models are trained on synthetic examples of your\n",
        "target wake word, it's a good idea to make sure that the examples\n",
        "sound correct. Type in your target wake word below, and run the\n",
        "cell to listen to it.\n",
        "Here are some tips that can help get the wake word to sound right:\n",
        "\n",
        "- If your wake word isn't being pronounced in the way\n",
        "you want, try spelling out the sounds phonetically with underscores\n",
        "separating each part.\n",
        "For example: \"hey siri\" --> \"hey_seer_e\".\n",
        "\n",
        "- Spell out numbers (\"2\" --> \"two\")\n",
        "\n",
        "- Avoid all punctuation except for \"?\" and \"!\", and remove unicode characters\n",
        "\n",
        "- Create multiple different ways of spelling out the same phrase phonetically. This helps the model learn various pronunciations and intonations people might use. For example, \"Hey Zelda\" could be varied as: ['hey Zelda', 'heyeh, zelda', 'hayeh, zelda', 'hey Zelda?', 'hay Zelduh', 'hay Zelda', 'hay Zelda?', 'hazelle_duh']\n",
        "  - This could potentially increase false activations so be careful with this\n",
        "  - You can check for duplicates in phenome identification with the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cbqBebHXjFD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from IPython.display import Audio\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "    !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
        "\n",
        "    # Install system dependencies\n",
        "    !pip install piper-phonemize\n",
        "    !pip install webrtcvad\n",
        "\n",
        "    if \"piper-sample-generator/\" not in sys.path:\n",
        "        sys.path.append(\"piper-sample-generator/\")\n",
        "\n",
        "    from generate_samples import generate_samples\n",
        "\n",
        "target_word = [\"hey tester\", \"hay tester\"]\n",
        "\n",
        "def text_to_speech(text):\n",
        "    generate_samples(text = text,\n",
        "                max_samples=1,\n",
        "                noise_scales=[1.0], noise_scale_ws=[1.0],\n",
        "                length_scales=[1.1],\n",
        "                # length_scales=[0.75, 1.0, 1.25],\n",
        "                output_dir = './', batch_size=1, auto_reduce_batch_size=True,\n",
        "                file_names=[\"test_generation.wav\"]\n",
        "                )\n",
        "# Test the pronounciation of the words confirm it is what you want\n",
        "text_to_speech(target_word[0])\n",
        "Audio(\"test_generation.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7MqwnF7oSr0"
      },
      "outputs": [],
      "source": [
        "# Mount at google drive for saving output files when running in the background\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWahyFO20_mh"
      },
      "outputs": [],
      "source": [
        "# Install all dependencies and download data\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# install openwakeword (full installation to support training)\n",
        "!git clone https://github.com/EthanEpp/openwakeword\n",
        "!pip install -e ./openwakeword\n",
        "!cd openwakeword\n",
        "\n",
        "# install other dependencies\n",
        "!pip install mutagen==1.47.0\n",
        "!pip install torchinfo==1.8.0\n",
        "!pip install torchmetrics==1.2.0\n",
        "!pip install speechbrain==0.5.14\n",
        "!pip install audiomentations==0.33.0\n",
        "!pip install torch-audiomentations==0.11.0\n",
        "!pip install acoustics==0.2.6\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow-cpu==2.8.1\n",
        "!pip install tensorflow_probability==0.16.0\n",
        "!pip install onnx_tf==1.10.0\n",
        "!pip install pronouncing==0.2.0\n",
        "!pip install datasets==2.14.6\n",
        "!pip install deep-phonemizer==0.0.19\n",
        "\n",
        "# Download required models (workaround for Colab)\n",
        "os.makedirs(\"./openwakeword/openwakeword/resources/models\")\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openwakeword/openwakeword/resources/models/embedding_model.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openwakeword/openwakeword/resources/models/embedding_model.tflite\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openwakeword/openwakeword/resources/models/melspectrogram.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openwakeword/openwakeword/resources/models/melspectrogram.tflite\n",
        "\n",
        "# Imports\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "    sys.path.append(\"piper-sample-generator/\")\n",
        "from generate_samples import generate_samples\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "import yaml\n",
        "import datasets\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Download all data\n",
        "\n",
        "## Download MIR RIR data (impulse responses)\n",
        "output_dir = \"./mit_rirs\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    !git lfs install\n",
        "    !git clone https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses\n",
        "    rir_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"./MIT_environmental_impulse_responses/16khz\").glob(\"*.wav\")]}).cast_column(\"audio\", datasets.Audio())\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    for row in tqdm(rir_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1]\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "## Download noise and background audio\n",
        "\n",
        "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
        "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
        "# For full-scale training, it's recommended to download the entire dataset from\n",
        "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
        "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
        "\n",
        "if not os.path.exists(\"audioset\"):\n",
        "    os.mkdir(\"audioset\")\n",
        "\n",
        "    fname = \"bal_train09.tar\"\n",
        "    out_dir = f\"audioset/{fname}\"\n",
        "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
        "    !wget -O {out_dir} {link}\n",
        "    !cd audioset && tar -xvf bal_train09.tar\n",
        "\n",
        "    output_dir = \"./audioset_16k\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
        "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "    for row in tqdm(audioset_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "# Free Music Archive dataset\n",
        "# https://github.com/mdeff/fma\n",
        "\n",
        "output_dir = \"./fma\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
        "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
        "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
        "        row = next(fma_dataset)\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "        i += 1\n",
        "        if i == n_hours*3600//30:\n",
        "            break\n",
        "\n",
        "# Download pre-computed openWakeWord features for training and validation, validation is made in house for licensing\n",
        "\n",
        "# training set (~2,000 hours from the ACAV100M Dataset)\n",
        "# See https://huggingface.co/datasets/davidscripka/openwakeword_features for more information\n",
        "if not os.path.exists(\"./openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"):\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
        "\n",
        "\n",
        "# validation set for false positive rate estimation\n",
        "if not os.path.exists(\"negative_features_dipco_ccv11.npy\"):\n",
        "    !wget https://huggingface.co/datasets/ethan3048/validfeaturescup/resolve/main/negative_features_dipco_ccv11.npy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters\n",
        "\n",
        "Each paramater controls a different aspect of training:\n",
        "- `number_of_examples` controls how many examples of your wakeword\n",
        "are generated.\n",
        "\n",
        "- `number_of_training_steps` controls how long to train the model.\n",
        "Similar to the number of examples, the default (10,000) usually works well\n",
        "but training longer usually helps.\n",
        "\n",
        "- `false_activation_penalty` controls how strongly false activations\n",
        "are penalized during the training process. Higher values can make the model\n",
        "much less likely to activate when it shouldn't, but may also cause it\n",
        "to not activate when the wake word isn't spoken clearly and there is\n",
        "background noise.\n",
        "\n",
        "- `custom_negative_phrases` similar to phrases chosen at the beginning but these should be variations that should not be accepted and will be used as adversarial. Example with target phrase \"Hey Zelda\". Negative phrases: [\"Hey Melda\", \"Say Zelda\", \"They sell the\"]\n",
        "  - This could potentially increase false rejections, so be careful with how these are selected. \\\n",
        "\n",
        "#Time For Different Scales on T4 Low Ram\n",
        "\n",
        "- number_of_examples = 30000, number_of_training_steps = 100000: ~1 - 1.5 hours\n",
        "  - Seemingly strong performance both on synthetic metrics and manual testing\n",
        "- number_of_examples = 50000, number_of_training_steps = 500000: ~2.5 - 3 hours\n",
        "  - Improved performance, but not relative to time increase\n",
        "- number_of_examples = 300000, number_of_training_steps = 1000000: ~10 - 14 hours\n",
        "  - Percentage of improvement is not relative to amount of time increase, but in theory should still do better\n",
        "\n",
        "\n",
        "When the model finishes training, you can navigate to the `my_custom_model` folder\n",
        "in the file browser on the left (click on the folder icon), and download\n",
        "the [your target wake word].onnx or  <your target wake word>.tflite files.\n",
        "They are also set to go to a google drive folder as navigated by destination phrase"
      ],
      "metadata": {
        "id": "VJeLHdpOpML5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgaKWIY6WlJ1"
      },
      "outputs": [],
      "source": [
        "# Load default YAML config file for training\n",
        "import yaml\n",
        "config = yaml.load(open(\"openwakeword/examples/custom_model.yml\", 'r').read(), yaml.Loader)\n",
        "\n",
        "# Modify values in the config and save a new version\n",
        "number_of_examples = 250\n",
        "number_of_training_steps = 1000\n",
        "false_activation_penalty = 3000\n",
        "config[\"target_phrase\"] = target_word\n",
        "config[\"model_name\"] = config[\"target_phrase\"][0].replace(\" \", \"_\")\n",
        "config[\"n_samples\"] = number_of_examples\n",
        "config[\"n_samples_val\"] = max(500, number_of_examples//10)\n",
        "config[\"steps\"] = number_of_training_steps\n",
        "config[\"target_accuracy\"] = 0.8\n",
        "config[\"target_recall\"] = 0.8\n",
        "config[\"output_dir\"] = \"./my_custom_model\"\n",
        "config[\"max_negative_weight\"] = false_activation_penalty\n",
        "# config[\"custom_negative_phrases\"] = []\n",
        "config[\"background_paths\"] = ['./audioset_16k', './fma']  # multiple background datasets are supported\n",
        "# config[\"false_positive_validation_data_path\"] = \"/content/drive/MyDrive/your_data_folder/negative_features_dipco_ccv11.npy\"\n",
        "config[\"false_positive_validation_data_path\"] = \"/content/negative_features_dipco_ccv11.npy\"\n",
        "config[\"feature_data_files\"] = {\"ACAV100M_sample\": \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"}\n",
        "\n",
        "with open('my_model.yaml', 'w') as file:\n",
        "    documents = yaml.dump(config, file)\n",
        "\n",
        "# Generate clips\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --generate_clips\n",
        "\n",
        "# Step 2: Augment the generated clips\n",
        "\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --augment_clips\n",
        "\n",
        "# Step 3: Train model\n",
        "\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --train_model\n",
        "\n",
        "# Manually save to tflite as this doesn't work right in colab\n",
        "def convert_onnx_to_tflite(onnx_model_path, output_path):\n",
        "    \"\"\"Converts an ONNX version of an openwakeword model to the Tensorflow tflite format.\"\"\"\n",
        "    # imports\n",
        "    import onnx\n",
        "    import logging\n",
        "    import tempfile\n",
        "    from onnx_tf.backend import prepare\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Convert to tflite from onnx model\n",
        "    onnx_model = onnx.load(onnx_model_path)\n",
        "    tf_rep = prepare(onnx_model, device=\"CPU\")\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tf_rep.export_graph(os.path.join(tmp_dir, \"tf_model\"))\n",
        "        converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, \"tf_model\"))\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        logging.info(f\"####\\nSaving tflite mode to '{output_path}'\")\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "    return None\n",
        "\n",
        "import shutil  # Import shutil for high-level file operations\n",
        "\n",
        "source_path = './my_custom_model'\n",
        "destination_phrase = \"hey_tester\"  # parent repo to each individual run\n",
        "destination_phrase_version = \"hey_tester_codeclean\"  # Base name for the version + iteration number\n",
        "\n",
        "base_destination_path = \"/content/drive/MyDrive/SoftAcuity Models/completed_models\"\n",
        "counter = 1\n",
        "destination_path = os.path.join(base_destination_path, destination_phrase, f\"{destination_phrase_version}_{counter}\")\n",
        "\n",
        "# Check if the directory exists and increment the counter until it doesn't\n",
        "while os.path.exists(destination_path):\n",
        "    counter += 1\n",
        "    destination_path = os.path.join(base_destination_path, destination_phrase, f\"{destination_phrase_version}_{counter}\")\n",
        "\n",
        "# Now, destination_path points to a non-existent directory\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Convert ONNX to TFLite if necessary\n",
        "convert_onnx_to_tflite(f\"{source_path}/{config['model_name']}.onnx\", f\"{destination_path}/{config['model_name']}.tflite\")\n",
        "\n",
        "# Copy only .onnx and .tflite files\n",
        "for filename in os.listdir(source_path):\n",
        "    if filename.endswith('.onnx') or filename.endswith('.tflite'):\n",
        "        shutil.copy(os.path.join(source_path, filename), destination_path)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Automatically download the trained model files\n",
        "files.download(f\"{destination_path}/{config['model_name']}.onnx\")\n",
        "files.download(f\"{destination_path}/{config['model_name']}.tflite\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}